{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def separate_features_target(df, target_column):\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    return X, y\n",
    "\n",
    "def identify_features(X):\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    return numerical_features, categorical_features\n",
    "\n",
    "def handle_duplicates(df):\n",
    "    df_no_duplicates = df.drop_duplicates()\n",
    "    return df_no_duplicates  \n",
    "\n",
    "def create_preprocessor(numerical_features,categorical_features):\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    return preprocessor\n",
    "\n",
    "def create_pipeline(preprocessor, classifier):\n",
    "    return Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "def train_pipeline(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "def evaluate_pipeline(pipeline, X_test, y_test):\n",
    "    accuracy = pipeline.score(X_test, y_test)\n",
    "    print(f'Model Accuracy: {accuracy}')\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_metrics(pipeline, X_test, y_test):\n",
    "    from sklearn.metrics import accuracy_score,precision_score,recall_score,log_loss\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    return {'accuracy': round(acc, 2), 'precision': round(prec, 2), 'recall': round(recall, 2)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment(experiment_name, metrics, pipeline, model_name,run_params=None):\n",
    "    import mlflow\n",
    "    from datetime import datetime\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\") \n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    run_name=experiment_name+str(datetime.now().strftime(\"%d-%m-%y\"))\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        if not run_params == None:\n",
    "            for param in run_params:\n",
    "                mlflow.log_param(param, run_params[param])\n",
    "            \n",
    "        for metric in metrics:\n",
    "            mlflow.log_metric(metric, metrics[metric])\n",
    "        \n",
    "        #mlflow.sklearn.log_model(pipeline, \"ml_pipline_2\")\n",
    "        mlflow.sklearn.log_model(pipeline, \"model\", registered_model_name=experiment_name)\n",
    "        #mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "        mlflow.set_tag(\"model\", model_name)\n",
    "            \n",
    "\n",
    "def train_and_log_with_mlflow(file_path, target_column, classifier, run_params,experiment_name):\n",
    "\n",
    "    # Read the dataset and perform initial data processing\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = handle_duplicates(df)\n",
    "    X, y = separate_features_target(df, target_column)\n",
    "    numerical_features, categorical_features = identify_features(X)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and train the pipeline\n",
    "    preprocessor = create_preprocessor(numerical_features, categorical_features)\n",
    "\n",
    "    # Set the classifier parameters\n",
    "    classifier.set_params(**run_params)\n",
    "\n",
    "    pipeline = create_pipeline(preprocessor, classifier)\n",
    "    train_pipeline(pipeline, X_train, y_train)\n",
    "    model_name = classifier.__class__.__name__\n",
    "    # Evaluate the pipeline\n",
    "    metrics = get_metrics(pipeline, X_test, y_test)\n",
    "\n",
    "    create_experiment(experiment_name, metrics, pipeline, model_name,run_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/02 10:29:06 INFO mlflow.tracking.fluent: Experiment with name 'mlflow_workinGGGG' does not exist. Creating a new experiment.\n",
      "c:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'mlflow_workinGGGG'.\n",
      "2024/02/02 10:29:12 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: mlflow_workinGGGG, version 1\n",
      "Created version '1' of model 'mlflow_workinGGGG'.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"train_data.csv\"\n",
    "target_column = \"cid\" \n",
    "classifier = RandomForestClassifier()\n",
    "run_params = {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "experiment_name = \"mlflow_workinGGGG\"\n",
    "\n",
    "train_and_log_with_mlflow(file_path, target_column, classifier, run_params,experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'creating_empty_registry' already exists. Creating a new version of this model...\n",
      "2024/01/04 11:43:39 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: creating_empty_registry, version 1\n",
      "Created version '1' of model 'creating_empty_registry'.\n"
     ]
    }
   ],
   "source": [
    "#Adding exsisting model to registry by using run id\n",
    "import mlflow\n",
    "with mlflow.start_run(run_name='add_model') as run:\n",
    "    result = mlflow.register_model(\n",
    "        \"runs:/66b0f7aacdd440b582ce2102cb2b5f55/model\",\n",
    "        \"creating_empty_registry\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RegisteredModel: aliases={}, creation_timestamp=1706849043036, description='', last_updated_timestamp=1706849043036, latest_versions=[], name='creating_empty_registry', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#While the method above creates an empty registered model with no version associated,\n",
    "import mlflow\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "client.create_registered_model(\"creating_empty_registry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df = handle_duplicates(df)\n",
    "X, y = separate_features_target(df, target_column)\n",
    "numerical_features, categorical_features = identify_features(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "model_name = \"mlflow_workinGGGG\"\n",
    "model_version = 1\n",
    "model = mlflow.pyfunc.load_model(\n",
    "    model_uri=f\"models:/{model_name}/{model_version}\"\n",
    ")\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_17120\\3454055362.py:2: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.9.2/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1706849952191, current_stage='Production', description='', last_updated_timestamp=1706850177556, name='mlflow_workinGGGG', run_id='04e4b924142244c5a6d9c86e30a40ee3', run_link='', source='file:///./artifactSS/2/04e4b924142244c5a6d9c86e30a40ee3/artifacts/model', status='READY', status_message='', tags={}, user_id='', version='1'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"mlflow_workinGGGG\",\n",
    "    version=1,\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\mlflow\\store\\artifact\\utils\\models.py:32: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.9.2/model-registry.html#migrating-from-stages\n",
      "  latest = client.get_latest_versions(name, None if stage is None else [stage])\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "model_name = \"mlflow_workinGGGG\"\n",
    "stage = 'Production'\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\n",
    "    model_uri=f\"models:/{model_name}/{stage}\"\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### to deploy model we have to set uri tracking basically where mlflow is running and we mlflow serve command\n",
    "###### set MLFLOW_TRACKING_URI=http://localhost:5000\n",
    "###### mlflow models serve --model-uri models:/mlflow_workinGGGG/Production -p 1234 --no-conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# Convert DataFrame to a dictionary\n",
    "data_dict = {\"dataframe_records\": X_test.to_dict(orient='records')}\n",
    "\n",
    "# Set the endpoint URL\n",
    "endpoint_url = \"http://localhost:1234/invocations\"\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(endpoint_url, json=data_dict)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    # Assuming the response is JSON, you can parse it\n",
    "    result = response.json()\n",
    "    print(result)\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "    print(\"Response content:\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "S3UploadFailedError",
     "evalue": "Failed to upload ./train_data.csv to artifact-store/train_data1.csv: An error occurred (SignatureDoesNotMatch) when calling the PutObject operation: The request signature we calculated does not match the signature you provided. Check your key and signing method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\boto3\\s3\\transfer.py:371\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[1;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 371\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\s3transfer\\futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\s3transfer\\futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\s3transfer\\tasks.py:139\u001b[0m, in \u001b[0;36mTask.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\s3transfer\\tasks.py:162\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\s3transfer\\upload.py:764\u001b[0m, in \u001b[0;36mPutObjectTask._main\u001b[1;34m(self, client, fileobj, bucket, key, extra_args)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fileobj \u001b[38;5;28;01mas\u001b[39;00m body:\n\u001b[1;32m--> 764\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\botocore\\client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\botocore\\client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (SignatureDoesNotMatch) when calling the PutObject operation: The request signature we calculated does not match the signature you provided. Check your key and signing method.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mS3UploadFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m s3_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_data1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Specify a valid S3 object name\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Upload the file to MinIO\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mupload_to_minio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminio_endpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminio_access_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminio_secret_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_file_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m, in \u001b[0;36mupload_to_minio\u001b[1;34m(local_file, bucket_name, minio_endpoint, minio_access_key, minio_secret_key, minio_secure, s3_file_name)\u001b[0m\n\u001b[0;32m     15\u001b[0m         s3_file_name \u001b[38;5;241m=\u001b[39m local_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Upload the file\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile uploaded successfully to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms3_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoCredentialsError:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\boto3\\s3\\inject.py:145\u001b[0m, in \u001b[0;36mupload_file\u001b[1;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Upload a file to an S3 object.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\mlflow_env\\Lib\\site-packages\\boto3\\s3\\transfer.py:377\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[1;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m S3UploadFailedError(\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to upload \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    379\u001b[0m             filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([bucket, key]), e\n\u001b[0;32m    380\u001b[0m         )\n\u001b[0;32m    381\u001b[0m     )\n",
      "\u001b[1;31mS3UploadFailedError\u001b[0m: Failed to upload ./train_data.csv to artifact-store/train_data1.csv: An error occurred (SignatureDoesNotMatch) when calling the PutObject operation: The request signature we calculated does not match the signature you provided. Check your key and signing method."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "def upload_to_minio(local_file, bucket_name, minio_endpoint, minio_access_key, minio_secret_key, minio_secure=True, s3_file_name=None):\n",
    "    # Create an S3 client for MinIO\n",
    "    s3 = boto3.client('s3', \n",
    "                      endpoint_url=minio_endpoint,\n",
    "                      aws_access_key_id=minio_access_key,\n",
    "                      aws_secret_access_key=minio_secret_key,\n",
    "                      verify=minio_secure)\n",
    "\n",
    "    try:\n",
    "        # If s3_file_name is not provided, use the local file name\n",
    "        if s3_file_name is None:\n",
    "            s3_file_name = local_file.split(\"/\")[-1]\n",
    "\n",
    "        # Upload the file\n",
    "        s3.upload_file(local_file, bucket_name, s3_file_name)\n",
    "        print(f\"File uploaded successfully to {bucket_name}/{s3_file_name}\")\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "\n",
    "# Replace with your MinIO details and file paths\n",
    "minio_endpoint = 'http://localhost:9000'  # Replace with your MinIO server endpoint\n",
    "minio_access_key = 'wEQOUzmCWt6iDBqopAvz'\n",
    "minio_secret_key = 'w6YAl5WpXsV0jAyjbebhTujKHnr8CErYJcJO2iFpp34cAy2kGr18sQUZJP6JZvkga7VKD012YHl42aDb'\n",
    "bucket_name = 'artifact-store'\n",
    "local_file_path = './train_data.csv'\n",
    "s3_file_name = 'train_data1.csv'  # Specify a valid S3 object name\n",
    "\n",
    "# Upload the file to MinIO\n",
    "upload_to_minio(local_file_path, bucket_name, minio_endpoint, minio_access_key, minio_secret_key, s3_file_name=s3_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
